{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bd6384-0e00-470f-8bb3-06cff5121328",
   "metadata": {},
   "source": [
    "## Entrainement de differents modèles pour calculer les probabilités de retour à l'emploi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d93bcd-d5eb-43c2-96e1-5b8e09834032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83554643-2f6f-4639-93a3-36b8c14dd357",
   "metadata": {},
   "source": [
    "### Rechargement du dataset Personnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd9e416-874c-484c-866d-4075b69dc76f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;31m7.1.9 and 7.2.17 are the last CDP runtime releases where Spark 2 is supported.\n",
      "Please migrate your Spark 2 applications to Spark 3.\n",
      "\n",
      "Updating Spark 2 applications for Spark 3:\n",
      "https://docs.cloudera.com/runtime/7.2.16/running-spark-applications/topics/spark-update-spark2-spark3.html\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# MASTER=\"yarn\"           # Si Spark pushdown sur CML\n",
    "MASTER=\"local[*]\"       # Si execute parallele sur CML\n",
    "\n",
    "spark = SparkSession.builder.appName(\"O_ingestion\").master(MASTER).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df59642-ea8a-4c2a-9a8b-32e4c8698840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = d6cedd98-b4fc-4910-9cf2-83c9e64183bd\n"
     ]
    }
   ],
   "source": [
    "sdf=spark.sql(\"select * from olivier.personnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf0f19-03de-4763-8222-1c100c0ee3bc",
   "metadata": {},
   "source": [
    "### Initialisation des experimentation MLFLOW pour l'Atelier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a07b175-bac3-4629-8e6b-5b69c552f201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b2f6d1-25a5-42f1-917f-cc3014ba9406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/12 17:33:34 INFO mlflow.tracking.fluent: Experiment with name 'Experimentations Atelier 1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/cdsw/.experiments/rgbu-b5q0-sqoq-olbo', creation_time=None, experiment_id='rgbu-b5q0-sqoq-olbo', last_update_time=None, lifecycle_stage='active', name='Experimentations Atelier 1', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment (\"Experimentations Atelier 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cd00f-9037-46df-93bd-e3c56ff440c4",
   "metadata": {},
   "source": [
    "## Lancement d'une entrainement Regression logistique de Spark ML Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148f6380-742f-4e28-8911-923a2c04303f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables = sdf.columns[1:-1]   # Toutes les variables moins la premier (id) et la derniere qui est la variable à étudier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3098e47a-13cb-4563-a905-9ff8a2b1d924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- sexe: string (nullable = true)\n",
      " |-- niveau_education: string (nullable = true)\n",
      " |-- duree_chomage: long (nullable = true)\n",
      " |-- experience_ant: long (nullable = true)\n",
      " |-- competence: string (nullable = true)\n",
      " |-- formation: boolean (nullable = true)\n",
      " |-- taux_chomage_local: double (nullable = true)\n",
      " |-- secteur_activite: string (nullable = true)\n",
      " |-- reseau_professionnel: long (nullable = true)\n",
      " |-- support_social: long (nullable = true)\n",
      " |-- retour_emploi: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba63f1-d64a-4add-907c-20ce7371b105",
   "metadata": {},
   "source": [
    "#### Transformation des variables categorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6fdbb0-435a-4366-bded-22a11ef2f1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0aa46f-c832-4269-8d1b-0890d4530a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'),\n",
       " ('sexe', 'string'),\n",
       " ('niveau_education', 'string'),\n",
       " ('duree_chomage', 'bigint'),\n",
       " ('experience_ant', 'bigint'),\n",
       " ('competence', 'string'),\n",
       " ('formation', 'boolean'),\n",
       " ('taux_chomage_local', 'double'),\n",
       " ('secteur_activite', 'string'),\n",
       " ('reseau_professionnel', 'bigint'),\n",
       " ('support_social', 'bigint')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.dtypes[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1039372c-45b1-4275-87ad-f8565541cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract les colonnes categories\n",
    "cat_cols=[]\n",
    "feature_cols=[]\n",
    "for colname, coltype in sdf.dtypes[1:-1]:\n",
    "    if ( coltype == 'string'):\n",
    "        cat_cols += [colname]\n",
    "    else:\n",
    "        feature_cols += [colname]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21d156d2-1dd8-4915-8f60-65316a5fb211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stages = [] \n",
    "\n",
    "# liste toutes les colonne, type de sdf moins la premiere (id) et derniere colonne (variable à etudier)\n",
    "\n",
    "for colname in cat_cols:\n",
    "\n",
    "   # Assigne un indice aux variable categorielle\n",
    "   indexer = StringIndexer(inputCol=colname, outputCol=colname + \"_index\") \n",
    "        \n",
    "   encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=colname + \"_vec\")  \n",
    "\n",
    "   # Enregistre les colonnes index à ajouter a la sdf\n",
    "   stages       += [indexer, encoder]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bb3f5e0-1fde-4503-9e8c-d3367d145ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Transform all features into a vector\n",
    "feature_cols = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "assemblerInputs = [c + \"_vec\" for c in cat_cols] + feature_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Create pipeline and use on dataset\n",
    "pipeline = Pipeline(stages=stages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11780e6b-48dc-4936-86bf-b946b3a921dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type string of column sexe is not supported.\\nData type string of column niveau_education is not supported.\\nData type string of column competence is not supported.\\nData type string of column secteur_activite is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o125.transform.\n: java.lang.IllegalArgumentException: Data type string of column sexe is not supported.\nData type string of column niveau_education is not supported.\nData type string of column competence is not supported.\nData type string of column secteur_activite is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:169)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:75)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_198/4062133586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type string of column sexe is not supported.\\nData type string of column niveau_education is not supported.\\nData type string of column competence is not supported.\\nData type string of column secteur_activite is not supported.'"
     ]
    }
   ],
   "source": [
    "sdf = pipeline.fit(sdf).transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eec41181-9a3b-425b-9d30-dfcbd4fd5f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adulte\n"
     ]
    }
   ],
   "source": [
    "age = \"25\"\n",
    "catégorie = \"Jeune\" if age == \"20\" else \"Adulte\"\n",
    "print(catégorie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03a53a22-5e27-43ea-96a8-33455e1abbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----------------+-------------+--------------+--------------------+---------+------------------+----------------+--------------------+--------------+-------------+\n",
      "|   id|age| sexe|niveau_education|duree_chomage|experience_ant|          competence|formation|taux_chomage_local|secteur_activite|reseau_professionnel|support_social|retour_emploi|\n",
      "+-----+---+-----+----------------+-------------+--------------+--------------------+---------+------------------+----------------+--------------------+--------------+-------------+\n",
      "|74947| 18|homme|        DOCTORAT|           24|             7|Equality and dive...|     true|16.510081672408976|           Group|                  16|             1|        false|\n",
      "|45820| 47|femme|         CAP/BEP|           20|             8|Scientist, physio...|    false|19.468590596632215|             Ltd|                   5|             1|         true|\n",
      "|77624| 67|homme|    BACCALAUREAT|            0|            18| Corporate treasurer|    false|14.882335710955337|             LLC|                  66|             9|        false|\n",
      "|69499| 69|femme|         BTS/DUT|            6|             1|Insurance claims ...|     true|7.9952612787047475|             PLC|                  73|             7|        false|\n",
      "|80723| 53|femme|         LICENSE|           16|             3|          Orthoptist|     true| 6.753279563330216|           Group|                  78|             1|        false|\n",
      "|64466| 32|femme|          MASTER|           14|            13|   Buyer, industrial|    false|16.228874824704818|             Inc|                  58|             9|         true|\n",
      "|13545| 70|femme|        DOCTORAT|            4|            13|Sports administrator|    false| 4.571837705397238|             Ltd|                  78|             6|        false|\n",
      "|47023| 55|homme|    BACCALAUREAT|           12|             2|Engineer, buildin...|    false|11.765000131798164|             Inc|                  76|             5|        false|\n",
      "|45302| 45|femme|         BTS/DUT|           11|            14|     Camera operator|     true|14.876911065867887|             PLC|                  92|             4|         true|\n",
      "|65724| 58|femme|    BACCALAUREAT|            7|            18| Programmer, systems|    false|12.087789957160792|             Inc|                  60|             3|        false|\n",
      "|22875| 65|homme|        DOCTORAT|           17|             4|Civil Service adm...|    false|   6.0725064869174|             LLC|                  26|             6|         true|\n",
      "|49221| 54|femme|        MAITRISE|           19|            11|      Science writer|     true| 7.215989848636237|             Ltd|                  35|             9|         true|\n",
      "|28104| 37|femme|         BTS/DUT|           22|            19|Furniture conserv...|    false| 16.80060455211897|             LLC|                  73|             8|        false|\n",
      "|35358| 37|homme|        MAITRISE|           20|            13|Trading standards...|    false| 4.791535396974328|             Ltd|                  31|             9|         true|\n",
      "|72878| 23|homme|         CAP/BEP|            4|            19|International aid...|     true| 2.289639409285348|             Ltd|                  93|             1|        false|\n",
      "| 5746| 36|femme|          MASTER|            5|            17|Pharmacist, hospital|    false|15.274876864558053|             PLC|                  82|             7|         true|\n",
      "|77900| 68|homme|         BTS/DUT|            7|            18|    Insurance broker|    false|18.702290970031406|             Inc|                  52|             2|        false|\n",
      "|47437| 48|femme|         CAP/BEP|           19|            12|Accountant, chart...|    false|12.593039525947498|        and Sons|                  57|             9|         true|\n",
      "|14352| 60|femme|        MAITRISE|            2|            12|   Financial adviser|     true|  15.4696405773153|        and Sons|                  20|             8|         true|\n",
      "|41501| 36|homme|    BACCALAUREAT|           24|             5|Product/process d...|     true|12.627123204198588|           Group|                  79|             0|         true|\n",
      "+-----+---+-----+----------------+-------------+--------------+--------------------+---------+------------------+----------------+--------------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(sdf).transform(sdf)\n",
    "model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a96a594-cae7-4bd4-8fc0-e9f1ed2088a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'inputCols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199/3255682800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstringIndexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], \\\n\u001b[0;32m---> 11\u001b[0;31m             outputCols=[c + \"_vec\"])    \n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mstages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Stages will be run later on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'inputCols'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    " \n",
    "cat_cols= [\"cut\", \"color\", \"clarity\"]\n",
    "stages = [] # Stages in Pipeline\n",
    "\n",
    "for c in cat_cols:\n",
    "    stringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], \\\n",
    "            outputCols=[c + \"_vec\"])    \n",
    "    stages += [stringIndexer, encoder] # Stages will be run later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343fab1-989b-434a-ac9b-14d75b133d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#specify predictor variables\n",
    "assembler = VectorAssembler(inputCols=['hours', 'prep_exams'], outputCol='features')\n",
    "\n",
    "#format DataFrame for linear regression\n",
    "data = assembler.transform(df)\n",
    "data = data.select('features', 'score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
