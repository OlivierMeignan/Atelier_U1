{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ef881d-122f-4dec-b715-16186576c5cb",
   "metadata": {},
   "source": [
    "## Génération de données aléatoires pour renseigner la table personnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9aea53a-de3a-4cd7-b18d-ba622c528a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f6b29-3834-4e33-94f8-abe1269635ee",
   "metadata": {},
   "source": [
    "### Demarrage de la session Spark. Si Spark pushdown active, verifier dans HUE qu'un Job Yarn est créé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8beb9ac5-ad2d-4479-ac21-6473212309b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0;31m7.1.9 and 7.2.17 are the last CDP runtime releases where Spark 2 is supported.\n",
      "Please migrate your Spark 2 applications to Spark 3.\n",
      "\n",
      "Updating Spark 2 applications for Spark 3:\n",
      "https://docs.cloudera.com/runtime/7.2.16/running-spark-applications/topics/spark-update-spark2-spark3.html\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# MASTER=\"yarn\"           # Si Spark pushdown sur CML\n",
    "MASTER=\"local[*]\"       # Si execute parallele sur CML\n",
    "\n",
    "spark = SparkSession.builder.appName(\"O_ingestion\").master(MASTER).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdca27a7-187f-4658-886b-92b95878481b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = ce5852d6-43eb-4779-aaf3-b0f5afcf4dd1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|           default|\n",
      "|information_schema|\n",
      "|           olivier|\n",
      "|               sys|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test de la connexion Spark\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e700b-a77e-4bb5-b927-fb2e50a4a76d",
   "metadata": {},
   "source": [
    "### Paramètres d'initialisation de la génération de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3952a55f-6418-4e07-8e72-e81155e55de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIGNES=10000      # Nombre de lignes de la dataframe Spark de destination\n",
    "PAS=1000          # Nombre de lignes de la dataframe Pandas intermédiaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae333ff-e120-4b8d-8d0f-dec6e1fa5159",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Génération de données aléatoire à partir d'un Faker Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd3b508-f4ec-4107-9fcf-d3c39ea0c276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1.0\n",
      "Iteration:  2.0\n",
      "Iteration:  3.0\n",
      "Iteration:  4.0\n",
      "Iteration:  5.0\n",
      "Iteration:  6.0\n",
      "Iteration:  7.0\n",
      "Iteration:  8.0\n",
      "Iteration:  9.0\n",
      "Iteration:  10.0\n",
      "CPU times: user 24.2 s, sys: 342 ms, total: 24.6 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fake = Faker()             # Initialisation du Faker\n",
    "\n",
    "df = pd.DataFrame()        # Création de la dataframe Pandas pour les données du Faker\n",
    "\n",
    "i = 0\n",
    "while i < LIGNES:\n",
    "          \n",
    "    i = i + 1\n",
    "\n",
    "    data = {\n",
    "        'id': fake.random_number(digits=5),\n",
    "        'age': random.randint(18, 70),\n",
    "        'sexe': random.choice(['homme', 'femme']),\n",
    "        'niveau_education': random.choice(['CAP/BEP', 'BACCALAUREAT', 'BTS/DUT',  'LICENSE', 'MAITRISE', 'MASTER', 'DOCTORAT']),\n",
    "        'duree_chomage': random.randint(0, 24),\n",
    "        'experience_ant': random.randint(0, 20),\n",
    "        'competence': fake.job(),\n",
    "        'formation': random.choice([True, False]),\n",
    "        'taux_chomage_local': random.uniform(2, 20),\n",
    "        'secteur_activite': fake.company_suffix(),\n",
    "        'reseau_professionnel': random.randint(0, 100),\n",
    "        'support_social': random.randint(0, 10),\n",
    "        'retour_emploi': random.choice([True, False])\n",
    "        }\n",
    "    \n",
    "    df = pd.concat( [df, pd.DataFrame(data, index=['id'])] )\n",
    "    \n",
    "    if (i%PAS == 0) : # A chaque remplissage de la dataframe pandas, chargement en bulk de la dataframe spark\n",
    "\n",
    "       sdf = spark.createDataFrame(df) if (i == PAS) else sdf.unionAll(spark.createDataFrame(df))\n",
    "         \n",
    "       df  = df.drop(df.index)\n",
    "    \n",
    "       print (\"Iteration: \", i/PAS)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b813e8e9-a811-41fc-9709-dd5147c8d95a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a07e47-715b-4536-928b-572d9d63e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE=\"olivier.personnes\"\n",
    "\n",
    "sdf.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bbb79-1ee4-4fa7-94e0-9fee19ccefba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568c7b2-5f37-4d4a-acf5-a49d14173780",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
